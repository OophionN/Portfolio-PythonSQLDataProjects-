#!/usr/bin/env python
# coding: utf-8

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Задача" data-toc-modified-id="Задача-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Задача</a></span><ul class="toc-item"><li><span><a href="#Общий-план-работы." data-toc-modified-id="Общий-план-работы.-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Общий план работы.</a></span></li></ul></li><li><span><a href="#Загрузка-библиотек,-датасета,-знакомство-с-датасетом,-первичная-обработка-и-проверки." data-toc-modified-id="Загрузка-библиотек,-датасета,-знакомство-с-датасетом,-первичная-обработка-и-проверки.-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Загрузка библиотек, датасета, знакомство с датасетом, первичная обработка и проверки.</a></span></li><li><span><a href="#Выгрузка-в-CSV" data-toc-modified-id="Выгрузка-в-CSV-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Выгрузка в CSV</a></span></li></ul></div>

# # Задача
# 
# Мы работаем аналитиком в Яндекс.Дзене. Почти всё время занимает анализ пользовательского взаимодействия с карточками статей.
# 
# Каждую карточку определяют её тема и источник (у него тоже есть тема). Примеры тем: «Красота и здоровье», «Россия», «Путешествия».
# 
# Пользователей системы характеризует возрастная категория. Скажем, «26-30» или «45+».
# 
# Есть три способа взаимодействия пользователей с системой:
#     
#     - Карточка отображена для пользователя (show);
#     - Пользователь кликнул на карточку (click);
#     - Пользователь просмотрел статью карточки (view).
# 
# Каждую неделю начинающие менеджеры Денис и Валерия задают вам одни и те же вопросы:
#     
#     - Сколько взаимодействий пользователей с карточками происходит в системе с разбивкой по темам карточек?
#     - Как много карточек генерируют источники с разными темами?
#     - Как соотносятся темы карточек и темы источников?
# 
#     На шестую неделю работы вы решаете, что процесс пора автоматизировать. Для Дениса и Валерии нужно сделать дашборд.
# Дашборд будет основываться на пайплайне, который будет брать данные из таблицы, в которых хранятся сырые данные, трансформировать данные и укладывать их в агрегирующую таблицу. Пайплайн будет разработан для вас дата-инженерами.
# 
#     Пообщавшись с менеджерами и администраторами баз данных, вы написали краткое ТЗ:
# 
#     - Бизнес-задача: анализ взаимодействия пользователей с карточками Яндекс.Дзен;
#     - Насколько часто предполагается пользоваться дашбордом: не реже, чем раз в неделю;
#     - Кто будет основным пользователем дашборда: менеджеры по анализу контента;
# 
#     Состав данных для дашборда:
#     - История событий по темам карточек (два графика - абсолютные числа и процентное соотношение);
#     - Разбивка событий по темам источников;
#     - Таблица соответствия тем источников темам карточек;
# 
#     По каким параметрам данные должны группироваться:
#     - Дата и время;
#     - Тема карточки;
#     - Тема источника;
#     - Возрастная группа;
# 
#     Характер данных:
#     - История событий по темам карточек — абсолютные величины с разбивкой по минутам;
#     - Разбивка событий по темам источников — относительные величины (% событий);
#     - Соответствия тем источников темам карточек - абсолютные величины;
#     - Важность: все графики имеют равную важность;
# 
#     Источники данных для дашборда: дата-инженеры обещали подготовить для вас агрегирующую таблицу dash_visits. Вот её структура:
#     - record_id — первичный ключ,
#     - item_topic — тема карточки,
#     - source_topic — тема источника,
#     - age_segment — возрастной сегмент,
#     - dt — дата и время,
#     - visits — количество событий.
#     - Таблица хранится в специально подготовленной для вас базе данных zen;
#     - Частота обновления данных: один раз в сутки, в полночь по UTC;
# 
# Какие графики должны отображаться и в каком порядке, какие элементы управления должны быть на дашборде отраженно в согласованном проекте дашборда.

# ## Общий план работы.
#     
#     - Мы конечно доверяем коллегам, но базовые проверки ДС никто не отменял.
#     - загрузим ДС, изучим его, посмотрим на данные, добавим столбцы (которых нам будет нехватать) и/или скорректируем типы данным
#     - сохраним датасет в формате csv и загрузим в Tableau
#     - построим графики (в Tableau), которые будут отвечать на вопросы коллег, которые потом используем в презентации.
#         - Сколько взаимодействий пользователей с карточками происходит в системе с разбивкой по темам карточек?
#         - Как много карточек генерируют источники с разными темами?
#         - Как соотносятся темы карточек и темы источников?
#     - после подготовки и перепроверки ДС, приступим к отрисовке дашборда для колллег, с учетом его согласованных параметров 

# # Загрузка библиотек, датасета, знакомство с датасетом, первичная обработка и проверки.

# In[1]:


import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sqlalchemy import create_engine


# In[2]:


db_config = {'user': 'praktikum_student', # имя пользователя
            'pwd': 'Sdf4$2;d-d30pp', # пароль
            'host': 'rc1b-wcoijxj3yxfsf3fs.mdb.yandexcloud.net',
            'port': 6432, # порт подключения
            'db': 'data-analyst-zen-project-db'} # название базы данных

connection_string = 'postgresql://{}:{}@{}:{}/{}'.format(db_config['user'],
                                                db_config['pwd'],
                                                db_config['host'],
                                                db_config['port'],
                                                db_config['db'])

engine = create_engine(connection_string)


# In[3]:


query = '''
            SELECT * FROM dash_visits
        '''


# In[4]:


dash_visits = pd.io.sql.read_sql(query, con = engine)


# In[5]:


dash_visits


# In[6]:


dash_visits.info ()


# In[7]:


def check(data):  # функция поиска дубликатов
    try:
        
        display('Проверка на дубликаты:')
        duplicates = data.duplicated()
        duplicate_rows = data.loc[duplicates]
        display(duplicate_rows.info())
        display(duplicate_rows)
        display('----------------------')
        display('Пропуски:')
        display(data.isna().sum())
        display('Пропуски в процентном отношении к всему датасету:')
        display(data.isna().sum() / len(data) * 100)

        num_rows_before = len(data)
        data.drop_duplicates(inplace=True)
        num_rows_after = len(data)
        num_rows_deleted = num_rows_before - num_rows_after
        percent_deleted = round(num_rows_deleted / num_rows_before * 100, 2)
        display(f'Удалено дубликатов: {num_rows_deleted} строк ({percent_deleted}% от всего датасета)')
        
    except Exception as e:
        print(f'ERROR: {e}')


# In[8]:


check (dash_visits)
# очевидных дубликатов нет 


# In[9]:


def check_unique(data):  # проверка на уникальность и оценка данных в датасете
    for col in data.select_dtypes(include=['object']):
        print(f"Уникальные значения в столбце {col}:")
        print(data[col].unique())
        print('---------------------')

    for col in data.select_dtypes(include=['datetime64']):
        print(f"Диапазон значений в столбце {col}:")
        print(f"Минимальное значение: {data[col].min()}")
        print(f"Максимальное значение: {data[col].max()}")
        print('---------------------')

    for col in data.select_dtypes(include=['int64', 'float64']):
        if len(data[col].unique()) > 10:
            print(f"В столбце {col} более 10 уникальных значений")
        else:
            print(f"Уникальные значения в столбце {col}:")
            print(data[col].unique())
        print('---------------------')


# In[10]:


check_unique (dash_visits)


# In[11]:


dash_visits ['item_topic'].nunique()


# In[13]:


dash_visits ['source_topic'].nunique()


# # Выгрузка в CSV
# Выгрузим файл в csv для использования в Tableau

# In[12]:


dash_visits.to_csv('visits_data.csv', index=False)

